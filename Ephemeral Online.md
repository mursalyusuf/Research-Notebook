So something I find interesting is the Internet, the idea of being able to communicate around the world through computer is only possible due to the inception and viral expansion of the internet and I want to make this my area of research. 
The issue though is that all information present on the web is ethereal (Pun intended). Servers or computer that host all th information on the web are will not last forever as computers die, websites go offline, urls change, etc. So to combat this i will use a three pronged approach: 1. Save all important websites and documents to my Zotero library as a screenshot at https://www.zotero.org/mursal/items 2. Use outwit hub to keep all information that I will be using for my project and thirdly using wget to mirror the entire site locally so that if there is a problem with Zotero i have a locally the website saved. While also using Api present on websites to save large portion of information when applicable.

My workflow for this will be as follows : 1. Compile a list of websites, pdf and documents I will be using for this final project 2. Save all pages being used to Zotero for future reference, 3. Use Outwit hub to create a detailed list of information that i will need for my project, also found it possible to use outwit hub on local files. The issue is that computers can be fickle when they choose to be, I've had to fix my Macbook 3 times in 4 years once due to my clumsy-ness and then twice due to hardware issues so that this doesn't happen all my work will be saved to my dropbox and git account.